{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to build a **NER** model ?\n",
        "1. dataset\n",
        "2. framework that supports building ner models -> spacy\n",
        "3. what is the shape of the dataset i need to make it work with spacy\n",
        "4. build training pipeline\n",
        "5. evaluate"
      ],
      "metadata": {
        "id": "x5k0SDkwpM84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x2CyEkfprczF",
        "outputId": "56ba8a54-e240-4c7b-8ee9-7ba0694203c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645,
          "referenced_widgets": [
            "fefedf83d317447a8fad1c1079114a9d",
            "db8ee6b3e06a405db9d74608b132600b",
            "108b0f0a8ee34951b328ad6bd17ccf0f",
            "a2cfe7f23e434b5eb0dbdb3edd57e528",
            "9540bd54ccd747ff9bace8b9a66896ac",
            "681341ccc62044e6823f688142b944f9",
            "449d02376f784fa0a5bdb247ecc365f2",
            "97fbed5b03ec42c68852da790fc8fa20",
            "ea2c2ce75dbf4ea1960c572038f9e913",
            "2856aa01521844408f4cb6bd5aa833e5",
            "cfc48c0d56d84374bdaf0b05a3accc1a",
            "e1ace587e54443d6b83f5774e64c158e",
            "fa01b1630e3e4dfdad1b16b61a816686",
            "3e4b2c6947be49a3ab4b0c9aad737cdd",
            "d8e34892782c4280b139724462694bcb",
            "82273b61572844329358cb114461c7d7",
            "9f2fce6ba0364411a1056c97011c6104",
            "38167db5b71c406d956507ee98c832c8",
            "dbd4cbd6556343eb9ea32f0aa0c8b6a7",
            "b57faf7765d04802905365e1d3b02f08",
            "c2478eff5f4d4c59b4f5651204de4596",
            "83b540156cdb4a24b7f6a46c7b86a39c",
            "b477abaf735e43c4baca428cc5e57175",
            "89fc973250e846698a46eaa70abdf616",
            "9790da642e9d456e8ce1b6edcd60bcf8",
            "9e0f2905ec0449db8e72221b8680b88a",
            "d1a48db9a433497083f0632a28978d11",
            "c1c2cae5de394e31b23518d836626472",
            "6744224092b74162874f2334e3215621",
            "a368bd391a6f4d5cbde1a674c14e253d",
            "44ac938b189e4e659b8f7c6265d0870c",
            "445b4dd9e91b45cbbe94561d68e9c0f7",
            "b8038d900746419f9b4b4e578a6bf65a",
            "e89ad9c759b740a2b68a61a6c5c65b51",
            "7a333890b9ca43e8996881aa9c016bc4",
            "ba4e73acfc4f4295830812b2e27f90ec",
            "aab4d1462a654da2bf76400342a39ec2",
            "689e8cdf3fb94f56835a18e1af0cd97f",
            "69425d1168354dffba7c7ccff0a10401",
            "1c5bcf43475545a78416cbd3ba8660df",
            "15a76ca44ffe4ecba9d460846fba209f",
            "5a04b8a66fe04bb5b8e4565a1ea107d0",
            "7b97a97e2d244ead89e2db0cd8c66cfb",
            "e1f22e42c9b742acbbb10f22d5b4ae9e",
            "13234e21ecbe498aa47b5a21ff61b6db",
            "ae168fd7091542fb9138921bece877aa",
            "6d9c4bb29f9f4462bbef29551a5c7ae7",
            "dbcaaad0ddbb49b980ddd131c0c84333",
            "386895d6314246f7a6eae5ce8c69ff0d",
            "85d85e4dcaef472a8519e96e369c4173",
            "3e62813799214795a10bd31de24dc99a",
            "ff58df2c16184ea195190c6832264ac4",
            "9c626b04106c4582838d823d8a35a4d3",
            "3f144ac5dda6453f81cf0053571c1a81",
            "f234e768b5ac44e48256e77cda346b44",
            "17cb5f9897174f2a8b8c9109d610abca",
            "60516b274f6248e6ad322f2f17876182",
            "6723e4323e1f4e54a1ce971ddc3d3b6c",
            "8386949269d443adbbbcfc0ecd696935",
            "7a28a0e393744bb498c6edccb3938404",
            "b7586ba99777484b97f8c2fa2ea13186",
            "6dd6d688e878491bb79d02b265ce4523",
            "f26eaf01b25747e6b87ea72bcb8e3388",
            "157ed3efb82e4c7cb9a0fbb4a0dc0d48",
            "91e66b63660f4807bac31cd2fcd110c2",
            "4229fe7426614caeb49ba984af077026"
          ]
        },
        "id": "q_Ti3DVEpJ3A",
        "outputId": "1a131302-a0a9-466b-8a6b-21f47e5dfaf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fefedf83d317447a8fad1c1079114a9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ace587e54443d6b83f5774e64c158e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for eriktks/conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eriktks/conll2003.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b477abaf735e43c4baca428cc5e57175"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e89ad9c759b740a2b68a61a6c5c65b51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13234e21ecbe498aa47b5a21ff61b6db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17cb5f9897174f2a8b8c9109d610abca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"eriktks/conll2003\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUX8LcqU2l0w",
        "outputId": "0353e8c2-8d03-4575-cef2-cbcf91261605"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config config.cfg --lang en --pipeline ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLUQkkaKp5r-",
        "outputId": "eb29b671-9ad3-47ff-afa1-a64415e8be4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
            "install the spacy-transformers package and re-run this command. The config\n",
            "generated now does not use transformers.\u001b[0m\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: en\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Convert dataset to spacy formate\n",
        "def convert_conll_to_spacy_format(example):\n",
        "    tokens = example['tokens']\n",
        "    tags = example['ner_tags']\n",
        "    labels = dataset['train'].features['ner_tags'].feature.names\n",
        "\n",
        "    entities = []\n",
        "    start = 0\n",
        "    text = \"\"\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if text:\n",
        "            text += \" \"\n",
        "            start += 1\n",
        "        token_start = start\n",
        "        token_end = start + len(token)\n",
        "        text += token\n",
        "        if labels[tag] != \"O\":\n",
        "            ent_type = labels[tag][2:]\n",
        "            if labels[tag].startswith(\"B-\"):\n",
        "                entities.append([token_start, token_end, ent_type])\n",
        "            elif labels[tag].startswith(\"I-\") and entities:\n",
        "                entities[-1][1] = token_end\n",
        "        start = token_end\n",
        "\n",
        "    return (text, {\"entities\": [tuple(ent) for ent in entities]})"
      ],
      "metadata": {
        "id": "tOQVAANfsx5Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training import Example\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "doc_bin = DocBin()\n",
        "for example in dataset[\"train\"]:\n",
        "    text, annotations = convert_conll_to_spacy_format(example)\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    doc_bin.add(example.reference)\n",
        "doc_bin.to_disk(\"train.spacy\")\n",
        "\n",
        "for example in dataset[\"validation\"]:\n",
        "    text, annotations = convert_conll_to_spacy_format(example)\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    doc_bin.add(example.reference)\n",
        "doc_bin.to_disk(\"dev.spacy\")"
      ],
      "metadata": {
        "id": "zcvvdYYH8RLq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy --output ./output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68GGoBH57mkk",
        "outputId": "7beb54ad-ded4-4ae0-87ec-e2362b48c15d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     44.28    0.00    0.00    0.00    0.00\n",
            "  0     200        277.27   2996.40   51.56   53.47   49.79    0.52\n",
            "  0     400        290.58   2281.65   68.98   68.74   69.22    0.69\n",
            "  0     600        240.43   1997.41   76.49   77.49   75.51    0.76\n",
            "  0     800        432.80   1988.48   80.59   81.30   79.89    0.81\n",
            "  0    1000       1128.01   2315.70   86.51   87.15   85.87    0.87\n",
            "  1    1200        461.59   2012.91   88.95   89.24   88.65    0.89\n",
            "  1    1400        478.27   1745.72   90.13   89.93   90.33    0.90\n",
            "  1    1600        635.11   2047.83   92.90   93.02   92.77    0.93\n",
            "  2    1800        730.35   2035.82   94.11   94.18   94.04    0.94\n",
            "  2    2000        735.74   1799.27   94.75   95.04   94.46    0.95\n",
            "  3    2200        884.35   1792.32   95.37   95.54   95.20    0.95\n",
            "  4    2400        993.18   1831.85   96.02   96.27   95.78    0.96\n",
            "  5    2600        950.96   1589.32   96.23   96.39   96.07    0.96\n",
            "  6    2800        837.28   1304.24   96.41   96.55   96.27    0.96\n",
            "  7    3000        859.02   1114.47   96.67   96.84   96.49    0.97\n",
            "  8    3200        873.37    973.13   96.68   96.67   96.69    0.97\n",
            "  9    3400        785.70    869.22   96.95   97.02   96.88    0.97\n",
            "  9    3600        851.84    797.20   97.13   97.22   97.04    0.97\n",
            " 10    3800        996.88    744.58   97.09   97.14   97.03    0.97\n",
            " 11    4000        683.57    638.38   97.22   97.36   97.08    0.97\n",
            " 12    4200        748.09    585.52   97.06   97.17   96.95    0.97\n",
            " 13    4400        779.36    567.05   97.04   97.16   96.91    0.97\n",
            " 14    4600        826.23    516.80   97.11   97.09   97.12    0.97\n",
            " 15    4800        743.36    491.17   97.20   97.27   97.12    0.97\n",
            " 16    5000        904.15    509.17   97.26   97.32   97.19    0.97\n",
            " 17    5200        859.43    480.70   97.25   97.33   97.17    0.97\n",
            " 18    5400        985.25    515.35   97.11   97.21   97.01    0.97\n",
            " 19    5600        952.98    521.34   97.36   97.55   97.16    0.97\n",
            " 19    5800        692.53    401.24   97.30   97.26   97.34    0.97\n",
            " 20    6000        933.65    408.45   97.32   97.39   97.25    0.97\n",
            " 21    6200        820.18    383.70   97.48   97.52   97.43    0.97\n",
            " 22    6400        967.08    347.65   97.36   97.43   97.29    0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"./output/model-best\")\n",
        "doc = nlp(\"Hugging Face is based in New York.\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label :{ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwFdOQQr9BSR",
        "outputId": "22f99e3f-7c00-4ec8-950e-6d0d043434e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Hugging Face, Label :ORG\n",
            "Entity: New York, Label :LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hi, I am Sarah and I live in New York. I have an Iphone 14pro\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label :{ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggkUKljjACsq",
        "outputId": "76948827-61d5-439a-a908-a8beaf865487"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Sarah, Label :PER\n",
            "Entity: New York, Label :LOC\n",
            "Entity: Iphone 14pro, Label :MISC\n"
          ]
        }
      ]
    }
  ]
}