{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing Techniques :"
      ],
      "metadata": {
        "id": "I6Kj31jQCKAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Lowercase Conversion :**"
      ],
      "metadata": {
        "id": "IQ8IZ2sIC359"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_conversion(text):\n",
        "  return text.lower()\n",
        "\n",
        "text = \"Hello NLP!\"\n",
        "lowercase_conversion(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oqua7FlmC3gd",
        "outputId": "94d99a26-c096-4682-ab1b-3446e4138b7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello nlp!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Stop Words Removal :**"
      ],
      "metadata": {
        "id": "iZ9aAp5oDpD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHyVnhkZD7O-",
        "outputId": "0c420624-014a-4382-90a9-346e8b04f6e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "remove_stopwords(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdN_-OfaQ2VJ",
        "outputId": "b051e6f5-f660-4738-bd3c-4b6aecb80fd9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example', 'sentence', 'stop', 'words', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Remove Punctuation**"
      ],
      "metadata": {
        "id": "d1-LMd9GRRbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  punctuation = r'[^\\w\\s]'\n",
        "  return re.sub(punctuation, '', text)\n",
        "\n",
        "text = \"Hello! This is an example sentence with punctuation.\"\n",
        "remove_punctuation(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y-A1swiRRGUd",
        "outputId": "d355c0f2-fc59-4157-a909-e85d0f2fed56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello This is an example sentence with punctuation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4. Regular Expressions (Regex)**"
      ],
      "metadata": {
        "id": "x3nvMOqdR7kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Match Simple Text\n",
        "text = \"I LOVE NLP\"\n",
        "pattern = r\"NLP\"\n",
        "re.search(pattern, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDK9aarNR4aC",
        "outputId": "86fec4a9-4851-4b90-854c-20de952e3764"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(7, 10), match='NLP'>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number Removal/Normalization\n",
        "def remove_numbers(text):\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "text = \"This is an example sentence with 123 numbers.\"\n",
        "remove_numbers(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BsEWAt4tS0g_",
        "outputId": "5aad1648-1c4f-4723-d74a-db00c1cd9a5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is an example sentence with  numbers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number Normalization\n",
        "def normalize_numbers(text, replacement='NUM'):\n",
        "  return re.sub(r'\\d+', replacement, text)\n",
        "\n",
        "text = \"This is an example sentence with 123 numbers.\"\n",
        "normalize_numbers(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WJYNAh-xTRbH",
        "outputId": "0a799886-9e27-4ace-fad9-869cd4232a39"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is an example sentence with NUM numbers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Noise Removal\n",
        "def remove_noise(text):\n",
        "  noise_pattern = r'[^a-zA-Z0-9\\s]'\n",
        "  text = re.sub(noise_pattern, '', text)\n",
        "\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text\n",
        "text = \"Special @#! characters & unicode     like 你好 should be removed.\"\n",
        "remove_noise(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "urnrinRBTZPn",
        "outputId": "5b27546e-550b-4071-9019-586f372956c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Special characters unicode like should be removed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Tokenization**"
      ],
      "metadata": {
        "id": "3LCeoWvuUdUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDtGR5HAT88I",
        "outputId": "38f13858-0f40-45d6-c5c4-b4914b1ccfc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  sentence_tokens = sent_tokenize(text)\n",
        "  return word_tokens, sentence_tokens\n",
        "\n",
        "text = \"This is an example sentence. It has multiple sentences.\"\n",
        "tokenize_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kupouIDNdw6A",
        "outputId": "2d0b7bf0-7af8-45c1-a71f-b1dcfc144f82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['This',\n",
              "  'is',\n",
              "  'an',\n",
              "  'example',\n",
              "  'sentence',\n",
              "  '.',\n",
              "  'It',\n",
              "  'has',\n",
              "  'multiple',\n",
              "  'sentences',\n",
              "  '.'],\n",
              " ['This is an example sentence.', 'It has multiple sentences.'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vl03En5gehKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try bert tokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def bert_tokenizer(text):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "bert_tokenizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "d2a6e3173663441c8836177da7426041",
            "c58935245a1844808a04ec3a66f3decd",
            "ef996ee8b58d4119b479033b5a025299",
            "60cc7d534eac46e7a13e647eddad40f1",
            "eaa18b8e62b64e9ea37cbb5d42a0136a",
            "f030a4a9cb2e4f6f9e29de4184b17b2d",
            "fe34634c0fc643529a441d8b1fa81871",
            "25febe0607bb47f7bc02e93a447c94f5",
            "733f128e74544f84baa0472992236027",
            "82dd5c0c2fed42feb768f8166fa9f3ee",
            "ca7be3dcfbad493b8478e10d0f5783ed",
            "c7a4318cc2ec494db24afbe4fc460695",
            "3441e6d69bce4b51a7e1a95e3309f521",
            "535a00c7449f4169851392d392c795fc",
            "3d57def354e246ac8a9539d5fedc16c6",
            "770bbb99e93c4ca48715e62c06303fe6",
            "0b70250d69e04a99a9b4bf0433735bbc",
            "44c44fcbf0ed40a3ae14e896cf0be2be",
            "399f54429ac641d4a5219190cad5515f",
            "a465aac03a8e476a804e314ab8db7bd0",
            "76eadbb7da7741828df5beb181fb7cda",
            "c5f2e73e80314f278aee46e247080009",
            "bd622cb040c14df7809a1e2e52807849",
            "2b9e29a13ef9483a8c6132aecd7ca70b",
            "ed93b44e0a2a4ce6a5e7e81279338026",
            "ed579ebe0ba44ca1bbca59cad4bb8adc",
            "abd0607e2fb9481490afeb7dbf774b88",
            "d9dec8ef412b4cfbaad06b273be0c70a",
            "794d0cc0397d4ad3920df565c372a499",
            "b20e67841f6b4cda9c28992f774bdf22",
            "031e9176225b4943b645e11f94dba891",
            "34687b50f52543beb68ba0c1cc8f55c1",
            "4582802849eb443f934e70bbc6e051da",
            "ab2b9468eb1149cba31df7b1bb1e4f69",
            "f0b39a3cab1a46ffb903167a61ea90ef",
            "8c4f349dfeb2439e8b2f96671a24bd4e",
            "4798a32f95554187a822362b7d467bad",
            "14ef833dc4e544c99bd93d6859b4e07e",
            "3f428a61a3bd4bc392a50878fd9038c5",
            "c1a070deac4d4825ba38fb17c3182f6c",
            "356028e0e70d475cb742faa6bc373f28",
            "931dcfde2ed143d4bee9493588371211",
            "5c2ebd430f634c8280f0ff2a3a505b67",
            "c95070e6f98c471aafb8213be94f6277"
          ]
        },
        "id": "HLqlx_1ceGS7",
        "outputId": "73981d44-8541-4071-b9f0-9262843882f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2a6e3173663441c8836177da7426041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7a4318cc2ec494db24afbe4fc460695"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd622cb040c14df7809a1e2e52807849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab2b9468eb1149cba31df7b1bb1e4f69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'an', 'example', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try GPT2 tokenizer\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "def gpt2_tokenizer(text):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "gpt2_tokenizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "4c6141a39759447a982a04f0590e9a98",
            "06e077d8873e4304955a47822a4b5103",
            "68357693dc144736bc6229017a2ce3cd",
            "8db28392f6cf4bd4849d5313f1447ee2",
            "50d9051529384f7e8445c97df2be915e",
            "cee32f6f390945a4bc35b67546cb7cf9",
            "a8dc8e432c6846fa93af0fc37504675f",
            "8e443c1dd57e4707abf76604d1853b75",
            "b76211608179411dacea86c3bceaf8ad",
            "b05cf13da6574e1cb58f0ebdc4ffd6ee",
            "3c60e57f64544683aafc5d5eb83eb009",
            "3ec68d41895d44f1bec4a592a3ea7157",
            "f9534fb4b8a1448eb11b471b85d9e410",
            "2259a91c0f4348a0955779fae6b70421",
            "90b88b610b5149f88bbecefb7a9c6930",
            "867cc7685a4249588a639a92fd13ef9b",
            "44d49fd2394b4cc2bcfad784d83c30b4",
            "dab6db9e321a4dc49b6c405e37f292ab",
            "05b80c0bbc3f4af4b394fe11cedaa99c",
            "f97bfe74d92c4a17a056bd3ba4ac1646",
            "ef5f1b4127ac44dcb8583bc50aab6a34",
            "b9168653bd074764998a1ae2fec55f99",
            "d203ec991e9348f68b7b040a5f161423",
            "7548c21e95774e12898aa9acde0b9e1d",
            "ed502ce3fabe40c39aae77590db89ec4",
            "208dc249ee224d4e96827aaa05f57989",
            "c4ec6bd339c44f5b8f66d79889dd7947",
            "fa06babacd33425fa75c518a4923f1f8",
            "bece6459473e4b10b0b5ddd57e5a35c8",
            "d3acb5f87e0848e585247d08f2668d6f",
            "c03e173d90a34f2291c4163c8c2b655b",
            "3256f8551f5b41b8ab74bd1f9da5cdab",
            "53e7df837f7b4ca4aa3f342ffc46f560",
            "eb1cce89c8a24258830d5a3613b5fe8b",
            "eb0015b8e3544fc989df42de4aeb4a30",
            "e59bbdfa65ac407c85e0ba367b0de02b",
            "05286961362c4d79880bc5aadca98a7c",
            "181e312b38ba4e6d8e25de2df9151dd9",
            "98eae997f45741dba1a74a452266c718",
            "d17a5c91380342158a48f2ac23c213e4",
            "0eb09c0ac1ea4d4eb74812ff956129cf",
            "a00c3b15e5fb403aa196c6a06ce969e6",
            "4bff0d54960e45af99e4f1ef413c9565",
            "c73a22fe5dd54b398e8874f62a91a86e",
            "d48ad9ba13534411b238a8c3b86621e6",
            "51a81a486b374085a4ce24d7788c52af",
            "2228a5a219d84513a22b1d6dca49d184",
            "fd3b539a82ac41dd95f2d6fba89b43c7",
            "3e83c446752641b08fdb477f6c5ee2fc",
            "db6cf05c11294173a73962558638a120",
            "9a776a288f1744829c4e054bf7bf722f",
            "c14c1c876a3d429ea0f3bb878c16aaa0",
            "8921a46f28434525864098e6833a86ad",
            "88b0d92686cf4b2d99e942b7bb56dea3",
            "bdb823db32e949cba040916e2840ac75"
          ]
        },
        "id": "7bbygcU7eF6I",
        "outputId": "a772b481-10be-4777-859f-eeea9342ae31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c6141a39759447a982a04f0590e9a98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec68d41895d44f1bec4a592a3ea7157"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d203ec991e9348f68b7b040a5f161423"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb1cce89c8a24258830d5a3613b5fe8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d48ad9ba13534411b238a8c3b86621e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'Ġis', 'Ġan', 'Ġexample', 'Ġsentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Stemming**"
      ],
      "metadata": {
        "id": "yLXjW3WVex7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "\n",
        "def stemming_text(text):\n",
        "  porter = PorterStemmer()\n",
        "  lancaster = LancasterStemmer()\n",
        "  snowball = SnowballStemmer('english')\n",
        "\n",
        "  porter_stems = [porter.stem(word) for word in text]\n",
        "  lancaster_stems = [lancaster.stem(word) for word in text]\n",
        "  snowball_stems = [snowball.stem(word) for word in text]\n",
        "\n",
        "  return porter_stems, lancaster_stems, snowball_stems\n",
        "\n",
        "text = [\"running\", \"runs\", \"ran\", \"run\"]\n",
        "stemming_text(text)\n",
        "\n",
        "\n",
        "porter_stems, lancaster_stems, snowball_stems = stemming_text(text)\n",
        "print(\"Porter Stems:\", porter_stems)\n",
        "print(\"Lancaster Stems:\", lancaster_stems)\n",
        "print(\"Snowball Stems:\", snowball_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMf6iF8qerkK",
        "outputId": "48c09f0f-78fb-4a20-b680-9015ba46cea4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stems: ['run', 'run', 'ran', 'run']\n",
            "Lancaster Stems: ['run', 'run', 'ran', 'run']\n",
            "Snowball Stems: ['run', 'run', 'ran', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Lemmatization**"
      ],
      "metadata": {
        "id": "5EvDpFDNfVWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4aliw2ofU1G",
        "outputId": "383d7520-a45e-4efc-d4b1-34da2e2740c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatization_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
        "  return lemmatized_words\n",
        "\n",
        "text = [\"running\", \"runs\", \"ran\", \"run\"]\n",
        "lemmatization_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt6fAdkrfP23",
        "outputId": "da7e8e08-d4a3-44d1-8058-acd561b016dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['running', 'run', 'ran', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Spell Correction**"
      ],
      "metadata": {
        "id": "kF8ZHr4HfpwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-0c10i8gJf6",
        "outputId": "8979e2ed-7165-41cb-edf0-c49b56dbe7a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "def correct_spelling(text):\n",
        "  spell = SpellChecker()\n",
        "  tokens = word_tokenize(text)\n",
        "  corrected_tokens = [spell.correction(token) for token in tokens]\n",
        "  return corrected_tokens\n",
        "\n",
        "text = \"I havv a good speling!\"\n",
        "correct_spelling(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3dSr9b8fj7B",
        "outputId": "53aa46fe-0674-41f9-a469-6cd8ab6614e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'have', 'a', 'good', 'spelling', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Text Normalization with TextBlob**"
      ],
      "metadata": {
        "id": "gXWAsomogSzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTwOdRibgON5",
        "outputId": "9a26e15a-1e7e-4c36-f8f1-3499fb84a8fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def normalize_text_with_textblob(text):\n",
        "  blob = TextBlob(text)\n",
        "  corrected_text = str(blob.correct())\n",
        "\n",
        "  sentiment = blob.sentiment\n",
        "\n",
        "  noun_phrases = blob.noun_phrases\n",
        "\n",
        "  return corrected_text, sentiment, noun_phrases\n",
        "\n",
        "text = \"The quik brown fox jumpd over the lazzy dog.\"\n",
        "# text = \"guuod feeling\"\n",
        "corrected, sentiment, noun_phrases = normalize_text_with_textblob(text)\n",
        "\n",
        "print(f\"Corrected: {corrected}\")\n",
        "print(f\"Sentiment: {sentiment}\")\n",
        "print(f\"Noun phrases: {noun_phrases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N13G0vq8gFNj",
        "outputId": "5618587f-785d-4599-913e-bc1193b19faf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected: The quick brown fox jumped over the lazy dog.\n",
            "Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Noun phrases: ['brown fox jumpd', 'lazzy dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "tYNa1tiKhx0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t8x9nnajp7Z",
        "outputId": "ef7449fa-dfaf-4377-bf6f-1c35b134b7a8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def preprocess_text(text):\n",
        "  \"\"\" Comprehensive text preprocessing pipeline\"\"\"\n",
        "  #Apply Lowercasing\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove Punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Remove numbers\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  # Remove whitespace\n",
        "  text = text.strip()\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove Stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "text = \"This is an example! The preprocessing pipeline removes punctuation, numbers (123), and stopwords.\"\n",
        "tokens = preprocess_text(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv26fcDIgyra",
        "outputId": "0ed8029a-48dd-429e-b7ac-653846c00d7d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exampl', 'preprocess', 'pipelin', 'remov', 'punctuat', 'number', 'stopword']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiOfE2pgkFqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}